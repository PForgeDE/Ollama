# ü§ñ Ollama Qwen 3 ‚Äî Unraid Docker Container

[![Build and Push Docker Image](https://github.com/PForgeDE/Ollama/actions/workflows/build-push.yml/badge.svg)](https://github.com/PForgeDE/Ollama/actions/workflows/build-push.yml)

Schlanker Ollama Docker Container f√ºr Unraid ‚Äî ohne vorinstalliertes Modell. Das Modell wird beim ersten Start automatisch gezogen und bei jedem Neustart auf Updates gepr√ºft. Kein GPU n√∂tig, l√§uft auf reiner CPU.

## ‚ú® Features

- ‚úÖ **Ollama** ready-to-run
- ‚úÖ **Qwen 3** wird beim Start automatisch gezogen & aktuell gehalten
- ‚úÖ **CPU-only** ‚Äî keine GPU n√∂tig
- ‚úÖ **OpenAI-kompatible API** auf Port 11434
- ‚úÖ **Persistente Modell-Speicherung** via Volume
- ‚úÖ **Unraid Community App Template** enthalten
- ‚úÖ **Frei w√§hlbares Modell** per Umgebungsvariable

---

## üöÄ Quick Start

### Docker Compose
```bash
docker compose up -d
```

### Docker Run
```bash
docker run -d \
  --name ollama-qwen3 \
  -p 11434:11434 \
  -v ollama_data:/root/.ollama \
  -e OLLAMA_MODEL=qwen3:1.7b \
  ghcr.io/pforgede/ollama:latest
```

### Unraid
1. Unraid Dashboard ‚Üí **Apps** ‚Üí **Community Applications**
2. Oder manuell: **Docker** ‚Üí **Add Container** ‚Üí Template URL:
   ```
   https://raw.githubusercontent.com/PForgeDE/Ollama/main/unraid/ollama-qwen3.xml
   ```

---

## ‚öôÔ∏è Umgebungsvariablen

| Variable | Standard | Beschreibung |
|----------|----------|--------------|
| `OLLAMA_MODEL` | `qwen3:1.7b` | Modell (wird beim Start gezogen) |
| `OLLAMA_HOST` | `0.0.0.0` | Listen-Adresse |
| `OLLAMA_KEEP_ALIVE` | `-1` | Modell im RAM halten (-1 = dauerhaft) |

**Verf√ºgbare Qwen 3 Modelle:**

| Modell | Gr√∂√üe | Empfehlung |
|--------|-------|------------|
| `qwen3:0.6b` | ~500 MB | Sehr schnell, einfache Aufgaben |
| `qwen3:1.7b` | ~1.1 GB | ‚≠ê Standard ‚Äî gut & schnell |
| `qwen3:4b` | ~2.5 GB | Mehr Qualit√§t |
| `qwen3:8b` | ~5 GB | Hohe Qualit√§t, mehr RAM n√∂tig |

---

## üîå Integration

### OpenClaw (AI Gateway)

In der `openclaw.json` Konfiguration:

```json
{
  "providers": {
    "ollama": {
      "baseUrl": "http://<SERVER-IP>:11434",
      "models": [
        {
          "id": "qwen3:1.7b",
          "name": "Qwen 3 1.7B (local)",
          "contextLength": 32768
        }
      ]
    }
  }
}
```

Modell in einer Session wechseln:
```
/model ollama/qwen3:1.7b
```

Als Fallback-Model f√ºr einen Agent:
```json
{
  "agents": {
    "myagent": {
      "model": "ollama/qwen3:1.7b"
    }
  }
}
```

> ‚ö†Ô∏è **Kein `apiKey`** f√ºr Ollama setzen ‚Äî das verursacht Fehler!

---

### Claude Code

Claude Code unterst√ºtzt Ollama via OpenAI-kompatiblem Endpoint:

```bash
# Umgebungsvariablen setzen
export ANTHROPIC_BASE_URL=http://<SERVER-IP>:11434/v1
export ANTHROPIC_API_KEY=ollama

# Claude Code starten
claude --model qwen3:1.7b
```

Oder dauerhaft in `~/.claude/settings.json`:
```json
{
  "env": {
    "ANTHROPIC_BASE_URL": "http://<SERVER-IP>:11434/v1",
    "ANTHROPIC_API_KEY": "ollama"
  }
}
```

---

### Open WebUI (Chat-Interface)

Das beliebteste Chat-UI f√ºr lokale Modelle:

```bash
docker run -d \
  --name open-webui \
  -p 3000:8080 \
  -e OLLAMA_BASE_URL=http://<SERVER-IP>:11434 \
  -v open-webui:/app/backend/data \
  ghcr.io/open-webui/open-webui:main
```

Dann im Browser: `http://<SERVER-IP>:3000`

F√ºr Unraid gibt es ebenfalls ein fertiges Community App Template f√ºr Open WebUI.

---

### Direkte API-Nutzung

**Chat (OpenAI-kompatibel):**
```bash
curl http://<SERVER-IP>:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"qwen3:1.7b","messages":[{"role":"user","content":"Hallo!"}]}'
```

**Ollama Native API:**
```bash
curl http://<SERVER-IP>:11434/api/generate \
  -d '{"model":"qwen3:1.7b","prompt":"Hallo!","stream":false}'
```

**Verf√ºgbare Modelle anzeigen:**
```bash
curl http://<SERVER-IP>:11434/api/tags
```

---

### Python

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://<SERVER-IP>:11434/v1",
    api_key="ollama",
)

response = client.chat.completions.create(
    model="qwen3:1.7b",
    messages=[{"role": "user", "content": "Erkl√§re Quantencomputer in 3 S√§tzen."}]
)
print(response.choices[0].message.content)
```

---

## üì¶ Image

```bash
# GitHub Container Registry (√∂ffentlich)
docker pull ghcr.io/pforgede/ollama:latest
```

---

## üîÑ Build & CI/CD

GitHub Actions baut automatisch bei jedem Push auf `main`:
- **Image:** `ghcr.io/pforgede/ollama:latest`
- **Plattformen:** `linux/amd64`, `linux/arm64`

---

## üìÑ Lizenz

MIT ‚Äî by [PForge](https://github.com/PForgeDE)
