<?xml version="1.0"?>
<Container version="2">
  <Name>ollama-qwen3</Name>
  <Repository>ghcr.io/pforgede/ollama:latest</Repository>
  <Registry>https://ghcr.io</Registry>
  <Network>bridge</Network>
  <MyIP/>
  <Shell>bash</Shell>
  <Privileged>false</Privileged>
  <Support>https://github.com/PForgeDE/Ollama</Support>
  <Project>https://ollama.com</Project>
  <Overview>Ollama LLM Server with Qwen 3 - CPU optimized for Unraid. Model is pulled automatically on first start and updated on every restart. No GPU required.</Overview>
  <Category>Productivity: Tools:</Category>
  <WebUI>http://[IP]:[PORT:11434]/</WebUI>
  <TemplateURL>https://raw.githubusercontent.com/PForgeDE/Ollama/main/unraid/ollama-qwen3.xml</TemplateURL>
  <Icon>https://ollama.com/public/ollama.png</Icon>
  <ExtraParams>--restart=unless-stopped</ExtraParams>
  <PostArgs/>
  <CPUset/>
  <DateInstalled/>
  <DonateText/>
  <DonateLink/>
  <Requires/>
  <Config Name="Ollama API Port" Target="11434" Default="11434" Mode="tcp" Description="Ollama API Port (OpenAI compatible)" Type="Port" Display="always" Required="true" Mask="false">11434</Config>
  <Config Name="Model Data" Target="/root/.ollama" Default="/mnt/user/appdata/ollama-qwen3" Mode="rw" Description="Storage for Ollama models and config" Type="Path" Display="always" Required="true" Mask="false">/mnt/user/appdata/ollama-qwen3</Config>
  <Config Name="Ollama Model" Target="OLLAMA_MODEL" Default="qwen3:1.7b" Description="Model to load (e.g. qwen3:1.7b, qwen3:4b, qwen3:8b)" Type="Variable" Display="always" Required="false" Mask="false">qwen3:1.7b</Config>
  <Config Name="Keep Alive" Target="OLLAMA_KEEP_ALIVE" Default="-1" Description="How long to keep model in RAM (-1 = forever)" Type="Variable" Display="advanced" Required="false" Mask="false">-1</Config>
</Container>
